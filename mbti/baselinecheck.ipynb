{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "types = ('ISTJ', 'ISFJ', 'INFJ', 'INTJ', \\\n",
    "\t\t 'ISTP', 'ISFP', 'INFP', 'INTP', \\\n",
    "\t\t 'ESTP', 'ESFP', 'ENFP', 'ENTP', \\\n",
    "\t\t 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ')\n",
    "\n",
    "gen_pop = {'ISTJ': 0.11, 'ISFJ': 0.09, 'INFJ': 0.04, 'INTJ': 0.05, \\\n",
    "\t\t   'ISTP': 0.05, 'ISFP': 0.05, 'INFP': 0.06, 'INTP': 0.06, \\\n",
    "\t\t   'ESTP': 0.04, 'ESFP': 0.04, 'ENFP': 0.08, 'ENTP': 0.06, \\\n",
    "\t\t   'ESTJ': 0.08, 'ESFJ': 0.09, 'ENFJ': 0.05, 'ENTJ': 0.05}\n",
    "\n",
    "n, ___ = df.shape\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "for mbti in df['type']:\n",
    "\tcounts[mbti] += 1\n",
    "\n",
    "limiting_type = None\n",
    "min_size = float('infinity')\n",
    "for mbti in counts.keys():\n",
    "\tsize = counts[mbti]/gen_pop[mbti]\n",
    "\tif size < min_size:\n",
    "\t\tmin_size = size\n",
    "\t\tlimiting_type = mbti\n",
    "\n",
    "dic = collections.defaultdict(list)\n",
    "for row in df.iterrows():\n",
    "\tdic[row[1]['type']].append(row)\n",
    "\n",
    "unclean_list = []\n",
    "\n",
    "\n",
    "with open('mbti_clean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\t\n",
    "\tfor mbti in gen_pop.keys():\n",
    "\t\tlist1 = dic[mbti]\n",
    "\t\tfor x in range(0, int(round(min_size*gen_pop[mbti]))):\n",
    "\t\t\twriter.writerow(list1[x][1])\t\n",
    "\t\tunclean_list.append(list1[int(round(min_size*gen_pop[mbti])) : len(list1)])\n",
    "\t\t\t\t\t\n",
    "with open('mbti_unclean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\tfor mbti in unclean_list:\n",
    "\t\tfor x in mbti:\n",
    "\t\t\twriter.writerow(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('mbti_unclean.csv')\n",
    "DIMENSIONS = ('IE', 'NS', 'TF', 'PJ')\n",
    "counts = collections.defaultdict(int)\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tfor mbti in df['type']:\n",
    "\t\tif letter_1 in mbti:\n",
    "\t\t\tcounts[letter_1] += 1\n",
    "\t\tif letter_2 in mbti:\n",
    "\t\t\tcounts[letter_2] += 1\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tif counts[letter_1] < counts[letter_2]:\n",
    "\t\tlimit = counts[letter_1]\n",
    "\telse: \n",
    "\t\tlimit = counts[letter_2]\n",
    "\tlist1 = []\n",
    "\tlist2 = []\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_1 in row[1]['type']:\n",
    "\t\t\tlist1.append(row[1]['posts'].split('|||'))\n",
    "\t\t\ti += 1\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_2 in row[1]['type']:\n",
    "\t\t\tlist2.append(row[1]['posts'].split('|||'))\t\n",
    "\t\t\ti += 1\n",
    "\twith open('train_' + letter_1 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list1:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\twith open('train_' + letter_2 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list2:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "letters = ('I', 'E', 'N', 'S', 'T', 'F', 'P', 'J')\n",
    "df = pd.read_csv('mbti_clean.csv')\n",
    "test = collections.defaultdict(list) \n",
    "\n",
    "for row in df.iterrows():\n",
    "\tposts = row[1]['posts'].split('|||')\n",
    "\ttest[row[1]['type']].append(posts)\n",
    "\n",
    "### write csv files for every every letter class and train vs. test class (16 total)\n",
    "for letter in letters: \n",
    "\tPATH = 'test_' + letter + '.csv'\n",
    "\twith open(PATH, 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor key in test.keys():\n",
    "\t\t\tif letter in key:\n",
    "\t\t\t\tfor hundred_posts in test[key]:\n",
    "\t\t\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\t\t\tif len(row) > 10: \n",
    "\t\t\t\t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1578cc91c38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1578cc91c38e>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_POST_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "DIMENSIONS = ['IE', 'NS', 'FT', 'PJ']\n",
    "CROSS_VALIDATION = False\n",
    "SAVE_MODEL = False\n",
    "\n",
    "for k in range(len(DIMENSIONS)):\n",
    "\tx_train = [] \n",
    "\ty_train = [] \n",
    "\tx_test = [] \n",
    "\ty_test = []\n",
    "\n",
    "\t### Read in data\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(0)\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(1)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(0)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(1)\n",
    "\n",
    "\ttypes = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "\ttypes = [x.lower() for x in types]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tstop_words = stopwords.words(\"english\")\n",
    "\n",
    "\tdef lemmatize(x):\n",
    "\t\tlemmatized = []\n",
    "\t\tfor post in x: \n",
    "\t\t\ttemp = post.lower() \n",
    "\t\t\tfor type_ in types: \n",
    "\t\t\t\ttemp = temp.replace(' ' + type_, '')\n",
    "\t\t\ttemp = ' '.join([lemmatizer.lemmatize(word) for word in temp.split(' ') if (word not in stop_words)])\n",
    "\t\t\tlemmatized.append(temp)\n",
    "\t\treturn np.array(lemmatized)\n",
    "\tdef preprocess(x):\n",
    "\t\tlemmatized = lemmatize(x)\n",
    "\t\ttokenized = tokenizer.texts_to_sequences(lemmatized)\n",
    "\t\treturn sequence.pad_sequences(tokenized, maxlen=MAX_POST_LENGTH)\n",
    "\n",
    "\tx_train = lemmatize(x_train)\n",
    "\tx_test = lemmatize(x_test)\n",
    "\n",
    "\t### Assign to dataframe\n",
    "\tdf = pd.DataFrame(data={'text': x_train, 'type': y_train})\n",
    "\tdf = df.sample(frac=1).reset_index(drop=True) ### Shuffle rows\n",
    "\n",
    "\n",
    "\t### Make pipeline\n",
    "\tpipeline = Pipeline([\n",
    "\t\t('vectorizer',  CountVectorizer(stop_words='english')), ### Bag-of-words\n",
    "\t\t('transformer', TfidfTransformer()), \n",
    "\t\t('classifier',  MultinomialNB()) ]) ### Performs best\n",
    "\t\t#('classifier',  SVC()) ])\n",
    "\t\t#('classifier',  DecisionTreeClassifier(max_depth=50)) ])\n",
    "\t\t#('classifier', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)) ])\n",
    "\t\t#('classifier',  Perceptron()) ])\n",
    "\t\t#('classifier',  KNeighborsClassifier(n_neighbors=2)) ])\n",
    "\n",
    "\t### Cross-validation classification (individual posts)\n",
    "\tif CROSS_VALIDATION: \n",
    "\t\tk_fold = KFold(n=len(df), n_folds=6)\n",
    "\t\tscores_k = []\n",
    "\t\tconfusion_k = np.array([[0, 0], [0, 0]])\n",
    "\t\tfor train_indices, test_indices in k_fold:\n",
    "\t\t\tx_train_k = df.iloc[train_indices]['text'].values\n",
    "\t\t\ty_train_k = df.iloc[train_indices]['type'].values\n",
    "\t\t\tx_test_k = df.iloc[test_indices]['text'].values\n",
    "\t\t\ty_test_k = df.iloc[test_indices]['type'].values\n",
    "\t\t\tpipeline.fit(x_train_k, y_train_k)\n",
    "\t\t\tpredictions_k = pipeline.predict(x_test_k)\n",
    "\t\t\tconfusion_k += confusion_matrix(y_test_k, predictions_k)\n",
    "\t\t\tscore_k = accuracy_score(y_test_k, predictions_k)\n",
    "\t\t\tscores_k.append(score_k)\n",
    "\t\twith open('cross_validation_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\t\tf.write('*** {}/{} TRAINING SET CROSS VALIDATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\tf.write('Total posts classified: {}\\n'.format(len(df)))\n",
    "\t\t\tf.write('Accuracy: {}\\n'.format(sum(scores_k)/len(scores_k)))\n",
    "\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\tf.write(np.array2string(confusion_k, separator=', '))\n",
    "\n",
    "\t### Test set classification (individual posts)\n",
    "\tpipeline.fit(df['text'].values, df['type'].values)\n",
    "\tpredictions = pipeline.predict(x_test) \n",
    "\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\tscore = accuracy_score(y_test, predictions)\n",
    "\twith open('test_set_post_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\tf.write('Confusion matrix: \\n')\n",
    "\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "\t###########################\n",
    "\t### USER CLASSIFICATION ###\n",
    "\t###########################\n",
    "\t\n",
    "\t### Read in user test set data\n",
    "\tx_test_a = []\n",
    "\tx_test_b = []\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tx_test_a.append(row)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader: \n",
    "\t\t\tx_test_b.append(row)\n",
    "\tx_test = x_test_a + x_test_b\n",
    "\n",
    "\t### Make predictions for users\n",
    "\t'''\n",
    "\tpredictions_a = []\n",
    "\tfor user_batch in x_test_a:\n",
    "\t\tpredictions_for_batch = pipeline.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_a.append(prediction)\n",
    "\tpredictions_b = []\n",
    "\tfor user_batch in x_test_b:\n",
    "\t\tpredictions_for_batch = pipeline.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_b.append(prediction)\n",
    "\tpredictions = predictions_a + predictions_b\n",
    "\t'''\n",
    "\n",
    "\tpredictions = []\n",
    "\tfor user in x_test: \n",
    "\t\tprob = float(sum(pipeline.predict_proba(preprocess(user)))/len(user))\n",
    "\t\tprediction = 1\n",
    "\t\tif prob < 0.5: \n",
    "\t\t\tprediction = 0\n",
    "\t\tpredictions.append(prediction)\n",
    "\n",
    "\t### Analyze user classification results\n",
    "\ty_test_a = [0 for ___ in predictions_a]\n",
    "\ty_test_b = [1 for ___ in predictions_b]\n",
    "\ty_test = y_test_a + y_test_b\n",
    "\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\tscore = accuracy_score(y_test, predictions)\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (USERS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\tf.write('Confusion matrix: \\n')\n",
    "\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "\t# Save model\n",
    "\tif SAVE_MODEL:\n",
    "\t\tpipeline.named_steps['classifier'].model.save('NB_classifier_{}.h5'.format(DIMENSIONS[k]))\n",
    "\t\tpipeline.named_steps['classifier'].model = None\n",
    "\t\tjoblib.dump(pipeline, 'NB_pipeline_{}.pkl'.format(DIMENSIONS[k]))\n",
    "\tdel pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
