{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "types = ('ISTJ', 'ISFJ', 'INFJ', 'INTJ', \\\n",
    "\t\t 'ISTP', 'ISFP', 'INFP', 'INTP', \\\n",
    "\t\t 'ESTP', 'ESFP', 'ENFP', 'ENTP', \\\n",
    "\t\t 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ')\n",
    "\n",
    "gen_pop = {'ISTJ': 0.11, 'ISFJ': 0.09, 'INFJ': 0.04, 'INTJ': 0.05, \\\n",
    "\t\t   'ISTP': 0.05, 'ISFP': 0.05, 'INFP': 0.06, 'INTP': 0.06, \\\n",
    "\t\t   'ESTP': 0.04, 'ESFP': 0.04, 'ENFP': 0.08, 'ENTP': 0.06, \\\n",
    "\t\t   'ESTJ': 0.08, 'ESFJ': 0.09, 'ENFJ': 0.05, 'ENTJ': 0.05}\n",
    "\n",
    "n, ___ = df.shape\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "for mbti in df['type']:\n",
    "\tcounts[mbti] += 1\n",
    "\n",
    "limiting_type = None\n",
    "min_size = float('infinity')\n",
    "for mbti in counts.keys():\n",
    "\tsize = counts[mbti]/gen_pop[mbti]\n",
    "\tif size < min_size:\n",
    "\t\tmin_size = size\n",
    "\t\tlimiting_type = mbti\n",
    "\n",
    "dic = collections.defaultdict(list)\n",
    "for row in df.iterrows():\n",
    "\tdic[row[1]['type']].append(row)\n",
    "\n",
    "unclean_list = []\n",
    "\n",
    "\n",
    "with open('mbti_clean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\t\n",
    "\tfor mbti in gen_pop.keys():\n",
    "\t\tlist1 = dic[mbti]\n",
    "\t\tfor x in range(0, int(round(min_size*gen_pop[mbti]))):\n",
    "\t\t\twriter.writerow(list1[x][1])\t\n",
    "\t\tunclean_list.append(list1[int(round(min_size*gen_pop[mbti])) : len(list1)])\n",
    "\t\t\t\t\t\n",
    "with open('mbti_unclean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\tfor mbti in unclean_list:\n",
    "\t\tfor x in mbti:\n",
    "\t\t\twriter.writerow(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('mbti_unclean.csv')\n",
    "DIMENSIONS = ('IE', 'NS', 'TF', 'PJ')\n",
    "counts = collections.defaultdict(int)\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tfor mbti in df['type']:\n",
    "\t\tif letter_1 in mbti:\n",
    "\t\t\tcounts[letter_1] += 1\n",
    "\t\tif letter_2 in mbti:\n",
    "\t\t\tcounts[letter_2] += 1\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tif counts[letter_1] < counts[letter_2]:\n",
    "\t\tlimit = counts[letter_1]\n",
    "\telse: \n",
    "\t\tlimit = counts[letter_2]\n",
    "\tlist1 = []\n",
    "\tlist2 = []\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_1 in row[1]['type']:\n",
    "\t\t\tlist1.append(row[1]['posts'].split('|||'))\n",
    "\t\t\ti += 1\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_2 in row[1]['type']:\n",
    "\t\t\tlist2.append(row[1]['posts'].split('|||'))\t\n",
    "\t\t\ti += 1\n",
    "\twith open('train_' + letter_1 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list1:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\twith open('train_' + letter_2 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list2:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "letters = ('I', 'E', 'N', 'S', 'T', 'F', 'P', 'J')\n",
    "df = pd.read_csv('mbti_clean.csv')\n",
    "test = collections.defaultdict(list) \n",
    "\n",
    "for row in df.iterrows():\n",
    "\tposts = row[1]['posts'].split('|||')\n",
    "\ttest[row[1]['type']].append(posts)\n",
    "\n",
    "### write csv files for every every letter class and train vs. test class (16 total)\n",
    "for letter in letters: \n",
    "\tPATH = 'test_' + letter + '.csv'\n",
    "\twith open(PATH, 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor key in test.keys():\n",
    "\t\t\tif letter in key:\n",
    "\t\t\t\tfor hundred_posts in test[key]:\n",
    "\t\t\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\t\t\tif len(row) > 10: \n",
    "\t\t\t\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 50)            125000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 145,251\n",
      "Trainable params: 145,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      " 266/1272 [=====>........................] - ETA: 50s - loss: 0.6954 - accuracy: 0.5070"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0df459046912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m### Test set classification (individual posts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "### Preprocessing variables\n",
    "DIMENSIONS = ['IE', 'NS', 'FT', 'PJ']\n",
    "MODEL_BATCH_SIZE = 128\n",
    "TOP_WORDS = 2500\n",
    "MAX_POST_LENGTH = 40\n",
    "EMBEDDING_VECTOR_LENGTH = 50\n",
    "\n",
    "### Learning variables\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.1\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "### Control variables\n",
    "CROSS_VALIDATION = False\n",
    "SAMPLE = False\n",
    "WORD_CLOUD = True\n",
    "\n",
    "for k in range(len(DIMENSIONS)):\n",
    "\t\n",
    "\t###########################\n",
    "\t### POST CLASSIFICATION ###\n",
    "\t###########################\n",
    "\n",
    "\t### Read in data\n",
    "\tx_train = [] \n",
    "\ty_train = [] \n",
    "\tx_test = [] \n",
    "\ty_test = []\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(0)\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(1)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(0)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(1)\n",
    "\n",
    "\t### Preprocessing (lemmatization, tokenization, and padding of input\n",
    "\ttypes = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "\t\t\t 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "\ttypes = [x.lower() for x in types]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tstop_words = stopwords.words(\"english\")\n",
    "\tdef lemmatize(x):\n",
    "\t\tlemmatized = []\n",
    "\t\tfor post in x: \n",
    "\t\t\ttemp = post.lower() \n",
    "\t\t\tfor type_ in types: \n",
    "\t\t\t\ttemp = temp.replace(' ' + type_, '')\n",
    "\t\t\ttemp = ' '.join([lemmatizer.lemmatize(word) for word in temp.split(' ') if (word not in stop_words)])\n",
    "\t\t\tlemmatized.append(temp)\n",
    "\t\treturn np.array(lemmatized)\n",
    "\ttokenizer = text.Tokenizer(num_words=TOP_WORDS, split=' ')\n",
    "\ttokenizer.fit_on_texts(lemmatize(x_train))\n",
    "\tdef preprocess(x):\n",
    "\t\tlemmatized = lemmatize(x)\n",
    "\t\ttokenized = tokenizer.texts_to_sequences(lemmatized)\n",
    "\t\treturn sequence.pad_sequences(tokenized, maxlen=MAX_POST_LENGTH)\n",
    "\n",
    "\t### Assign to dataframe and shuffle rows\n",
    "\tdf = pd.DataFrame(data={'x': x_train, 'y': y_train})\n",
    "\tdf = df.sample(frac=1).reset_index(drop=True) ### Shuffle rows\n",
    "\tif SAMPLE:\n",
    "\t\tdf = df.head(10000) ### Small sample for quick runs\n",
    "\t\n",
    "\t### Load glove into memory for embedding\n",
    "\tembeddings_index = dict()\n",
    "\twith open('glove.6B.50d.txt') as f: \n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split()\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tembeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "\tprint('Loaded {} word vectors.'.format(len(embeddings_index)))\n",
    "\t\n",
    "\t### Create a weight matrix for words\n",
    "\tembedding_matrix = np.zeros((TOP_WORDS, EMBEDDING_VECTOR_LENGTH))\n",
    "\tfor word, i in tokenizer.word_index.items():\n",
    "\t\tif i < TOP_WORDS: \n",
    "\t\t\tembedding_vector = embeddings_index.get(word)\n",
    "\t\t\tif embedding_vector is not None:\n",
    "\t\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\n",
    "\t### Construct model\n",
    "\twith tf.device('/gpu:0'):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Embedding(TOP_WORDS, EMBEDDING_VECTOR_LENGTH, input_length=MAX_POST_LENGTH, \n",
    "\t\t\t\t\t\t\tweights=[embedding_matrix], mask_zero=True, trainable=True))\n",
    "\t\t#model.add(SimpleRNN(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(GRU(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\tmodel.add(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(Bidirectional(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros')))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t\toptimizer = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\t\tprint(model.summary())\n",
    "\n",
    "\t\t### Cross-validation classification (individual posts)\n",
    "\t\tif CROSS_VALIDATION: \n",
    "\t\t\tk_fold = KFold(n=len(x_train), n_folds=5)\n",
    "\t\t\tscores_k = []\n",
    "\t\t\tconfusion_k = np.array([[0, 0], [0, 0]])\n",
    "\t\t\tfor train_indices, test_indices in k_fold:\n",
    "\t\t\t\tx_train_k = df.iloc[train_indices]['x'].values\n",
    "\t\t\t\ty_train_k = df.iloc[train_indices]['y'].values\n",
    "\t\t\t\tx_test_k = df.iloc[test_indices]['x'].values\n",
    "\t\t\t\ty_test_k = df.iloc[test_indices]['y'].values\n",
    "\t\t\t\tmodel.fit(preprocess(x_train_k), y_train_k, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\t\t\tpredictions_k = model.predict_classes(preprocess(x_test_k))\n",
    "\t\t\t\tconfusion_k += confusion_matrix(y_test_k, predictions_k)\n",
    "\t\t\t\tscore_k = accuracy_score(y_test_k, predictions_k)\n",
    "\t\t\t\tscores_k.append(score_k)\n",
    "\t\t\twith open('cross_validation_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\t\t\tf.write('*** {}/{} TRAINING SET CROSS VALIDATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_train)))\n",
    "\t\t\t\tf.write('Accuracy: {}\\n'.format(sum(scores_k)/len(scores_k)))\n",
    "\t\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\t\tf.write(np.array2string(confusion_k, separator=', '))\n",
    "\t\t\n",
    "\t\t### Test set classification (individual posts)\n",
    "\t\tmodel.fit(preprocess(df['x'].values), df['y'].values, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\tpredictions = model.predict_classes(preprocess(x_test)) \n",
    "\t\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\t\tscore = accuracy_score(y_test, predictions)\n",
    "\t\twith open('test_set_post_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "\t\t### Get most a-like/b-like sentences \n",
    "\t\tif WORD_CLOUD:\n",
    "\t\t\tNUM_EXTREME_EXAMPLES = 500\n",
    "\t\t\tprobs = model.predict_proba(preprocess(x_test))\n",
    "\t\t\tscores = []\n",
    "\t\t\tindices = []\n",
    "\t\t\tfor i, prob in enumerate(probs, 0): \n",
    "\t\t\t\tscores.append(prob[0])\n",
    "\t\t\t\tindices.append(i)\n",
    "\t\t\tsorted_probs = sorted(zip(scores, indices))\n",
    "\t\t\tmin_prob_indices = sorted_probs[:NUM_EXTREME_EXAMPLES]\n",
    "\t\t\tmax_prob_indices = sorted_probs[-NUM_EXTREME_EXAMPLES:]\n",
    "\t\t\tlemmatized = lemmatize(x_test)\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][0]), 'w') as f: \n",
    "\t\t\t\tfor prob, i in min_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][1]), 'w') as f: \n",
    "\t\t\t\tfor prob, i in max_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\n",
    "\t\t### Save model and tokenizer for future use\n",
    "\t\tmodel.save('model_{}.h5'.format(DIMENSIONS[k]))\n",
    "\t\twith open('tokenizer_{}.pkl'.format(DIMENSIONS[k]), 'wb') as f:\n",
    "\t\t\tpickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t###########################\n",
    "\t### USER CLASSIFICATION ###\n",
    "\t###########################\n",
    "\t\n",
    "\t### Read in user test set data\n",
    "\tx_test_a = []\n",
    "\tx_test_b = []\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tx_test_a.append(row)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader: \n",
    "\t\t\tx_test_b.append(row)\n",
    "\tx_test = x_test_a + x_test_b\n",
    "\n",
    "\t### Make predictions for users\n",
    "\tpredictions_a = []\n",
    "\tfor user_batch in x_test_a:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_a.append(prediction)\n",
    "\tpredictions_b = []\n",
    "\tfor user_batch in x_test_b:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_b.append(prediction)\n",
    "\tpredictions = predictions_a + predictions_b\n",
    "\n",
    "\t### Analyze user classification results\n",
    "\ty_test_a = [0 for ___ in predictions_a]\n",
    "\ty_test_b = [1 for ___ in predictions_b]\n",
    "\ty_test = y_test_a + y_test_b\n",
    "\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\tscore = accuracy_score(y_test, predictions)\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (USERS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\tf.write('Confusion matrix: \\n')\n",
    "\t\tf.write(np.array2string(confusion, separator=', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(DIMENSIONS)):\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'r') as f: \n",
    "\t\t\tprint(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
